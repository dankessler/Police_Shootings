* Introduction
** The Problem
US has recently witness a number of high-profile deaths of African-Americans at the hands of the police
- Michael Brown
- Eric Garner
- Walter Scott

There is a pressing need to study this phenomenon quantitatively. Why?
- Case reports are insufficient and could even be misleading
- Better understand the causes
- Help shape solutions
** An Important Step Forward
** Key Results
** Present work

Main Questions
1. Are Ross's main results replicated in WashPo database?
2. Is there temporal stability in spatial variation in risk ratios?
* The Method: Bayesian Methods & Stan
** Background: Bayesian Methods
** Background: Stan
** Obtaining Stan
** RStan
* Data Processing: 2 Datasets
** Goal
Our goal is to arrive at a dataset where each row encodes one county. We will have four outcome measures per county (Period X Race). We also require the following covariates
- Black Population (for use as offset)
- White Population (for use as offset)
- Total Population
- B:W Population
- B:W Arrest Rate for Assault

We also have a dataset of county-level covariates, obtained from Dr. Ross's github repository. This isn't provided in raw format, but is rather the result of merging county-level shooting rates with some unprovided demographic file. Here, I extract only the demographics of interest.
** Load Source Data Files

#+BEGIN_SRC R :session :results none :export code
  cases.uspsd <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/U.S. Police Shootings Data (Cleaned).csv')

  cases.wapo <- read.csv('../submodules/data-police-shootings/fatal-police-shootings-data.csv')

  mapfile <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/MapFileData-WithCountyResultsAndCovariates.csv')

  mapfile$BW.PopRatio <- mapfile$BAC_TOT / mapfile$WA_TOT
  mapfile$BlackAssaultArrest <- mapfile$AssaultsBlack.sum / mapfile$BAC_TOT
  mapfile$WhiteAssaultArrest <- mapfile$AssaultsWhite.sum / mapfile$WA_TOT
  mapfile$BW.AssaultRatio <- mapfile$BlackAssaultArrest / mapfile$WhiteAssaultArrest

  county.demographics <- data.frame(
      County.FIPS.Code = mapfile$County.FIPS.Code,
      County.Name = mapfile$County.Name,
      State = mapfile$State,
      BlackPop = mapfile$BAC_TOT,
      WhitePop = mapfile$WA_TOT,
      TotPop = mapfile$TOT_POP,
      BW.PopRatio = mapfile$BW.PopRatio,
      BW.AssaultRatio = mapfile$BW.AssaultRatio)

  rm(mapfile) # tidy up the namespace
#+END_SRC

** Map to Counties
*** USPSD
We can skip this step, because Cody Ross's github already provides data in a more processed format that we'll load downstream.

*** WaPo
The Washington Post Database has info at a city-state level. In order to merge the WaPo database with the USPSD database, we must annotate each entry with the county and associated code.

First, let's define some helpful functions. We rely on the geo.lookup function from the acs package. However, it sometimes returns multiple (or no) county mappings for certain entities, which we must correct by hand using a hand-tuned map.  In an attempt to make future City, State -> County mapping, we've made the hand-tuned map available on [[https://github.com/dankessler/city_county_map][github]].

#+BEGIN_SRC R :session :results none :export code
  library(acs)

  addCounties <- function(df){
      names(df)[grepl('state',names(df))] <- 'State' # shift case to match Cody
      cities <- unique(df[,c('city','State')])
      handmap <- read.csv('/home/kesslerd/repos/Analysis/PoliceShootings/city_county_map/HandMappings.csv')
      cities$County.Name <- mapply(getCounty,cities$State,cities$city,MoreArgs=list(handmap=handmap))
      cities$County.Name <- unlist(cities$County.Name)
      return(merge(df,cities,all.x=TRUE))
  }

  getCounty <- function(state,city,handmap){
      hand.candidate <- handmap[handmap$city==city & handmap$State==state,]
      if (nrow(hand.candidate)==1){
          return(hand.candidate['Hand_CountyName'])
      }

      candidates <- geo.lookup(state=state,place=city)
      if (nrow(candidates) < 2){ # confirm that we have some hits
          return ('NoMatch')
      }
      candidates <- candidates[-1,] # drop the first hit which is null
      candidates <- unique(candidates) # deal with duplicates

      dists <- adist(city,candidates[,'place.name'])
      shortest <- min(dists)
      dups <- sum(dists==shortest)
      if (dups>1){
          return('MultiRowMatch')
      }
      bestind <- which.min(dists)
      county <- candidates[bestind,'county.name']
      return(county)
  }
#+END_SRC

Next, we'll use them to clean up the WaPo dataset.

#+BEGIN_SRC R :session :results none :export code
  cases.wapo <- addCounties(cases.wapo)
#+END_SRC

** Summarize by County, Armed Status, and Race
Again, the treatment for each file is a bit different, as they structure their incident data differently.
*** USPSD
Conveniently, Cody's repository provides the file in a format with summaries already calculated and covariates included. For convenience sake, we're going to jump to this point in the stream and extract only the critical pieces of information, so that we can merge with WaPo data and add covariates later.

#+BEGIN_SRC R :session :results none :exports code
  cases.uspsd <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/MapFileData-WithCountyResultsAndCovariates.csv')

  uspsd <- data.frame(
      State = cases.uspsd$State,
      County.Name = cases.uspsd$County.Name,
      B = cases.uspsd$BlackUnarmed,
      W = cases.uspsd$WhiteUnarmed)
#+END_SRC
*** WaPo
The WaPo data is quite granular with respect to what weapon (if any) was carried by the civilian. Because we are interested specifically in unarmed civilians, we only count cases annotated as "unarmed."
#+BEGIN_SRC R :session :results none :exports code
  library(reshape2)

  wapo <- dcast(cases.wapo, State + County.Name ~ race, subset = .(race %in% c('B','W') & cases.wapo$armed=='unarmed'),fun.aggregate=length)

#+END_SRC
** Merge USPSD and WaPo
Next we combine the two files, with suffixes such that we can identify the source. There are a small number of counties that appear in only one dataset. In these cases, we replace the missing data with 0 shootings, since this means there were none reported.

#+BEGIN_SRC R :session :exports code :results none
  unarmed.counts <- merge(wapo,uspsd,by=c('State','County.Name'),suffixes=c('.wapo','.uspsd'))

  # change NAs to 0s
  unarmed.counts[,c('B.wapo','W.wapo','B.uspsd','W.uspsd')] <- apply(unarmed.counts[,c('B.wapo','W.wapo','B.uspsd','W.uspsd')],c(1,2),function(x){ifelse(is.na(x),0,x)})
#+END_SRC
** Merge with County-Level Covariates
For all of the counties that appear in our merged file, we add county level demographics.

#+BEGIN_SRC R :session :exports code :results none
  unarmed.counts <- merge(unarmed.counts,county.demographics,all.x=TRUE,all.y=FALSE)
#+END_SRC
** Clean Dataset
Drop counties with partial coverage.

#+BEGIN_SRC R :session :exports code :results none
  unarmed.counts <- unarmed.counts[complete.cases(unarmed.counts),]
#+END_SRC 
* Reproduce & Replicate
** Stan Code

* Repeating Cody Ross Analysis

* Replication with WaPo Data
* Poisson Models
** Stan Code
#+NAME: stan-poisson
#+BEGIN_SRC stan
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cw1[nc];
    int<lower=0> pDemo; // number of demographic predictors
    vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    //  int<lower=0> p;
    vector[pDemo + 5] x[nc,2]; // nc, race, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2]; // number of shootings for county i, race j
    //  p = pDemo + 5; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1:pDemo] = xDemo[i]; // black year 1
      x[i,2,1:pDemo] = rep_vector(0,pDemo); // white year 1

      x[i,1,pDemo+1] = log(xOffset[i,1]); // black population (offset), log scale for offset
      x[i,2,pDemo+1] = log(xOffset[i,2]); // white population (offset), log scale for offset

      x[i,1,pDemo+2] = 1; // race (black)
      x[i,2,pDemo+2] = 0; // race (black)


      x[i,1,pDemo+3] = 1; // intercept
      x[i,2,pDemo+3] = 1; // intercept

      x[i,1,pDemo+4] = 1; // dummy code for county specific beta
      x[i,2,pDemo+4] = 1; // dummy code for county specific beta

      x[i,1,pDemo+5] = 1; // dummy code for county:race specific beta
      x[i,2,pDemo+5] = 0; // dummy code for county:race specific beta

    }



    for (i in 1:nc){
      C[i,1] = Cb1[i];
      C[i,2] = Cw1[i];
    }


  }
  parameters {
    row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    real beta_CountyTime[nc]; // ranfx for county:time
    real beta_RaceCountyTime[nc]; //ranfx for county:time:race
    real<lower=0> SigmaCountyTime;
    real<lower=0> SigmaRaceCountyTime;

  }
  transformed parameters {
    row_vector[pDemo+5] theta[nc,2]; // setup a theta predictor for each observation
    real<lower=0> lambda[nc,2] ; // lambda defines the poisson
    for (i in 1:nc){
      theta[i,1,1:pDemo] = beta_RaceDemo; // black year 1
      theta[i,2,1:pDemo] = beta_RaceDemo; // white year 2

      theta[i,1,pDemo+1] = 1; // population (offset)
      theta[i,2,pDemo+1] = 1; // population (offset)

      theta[i,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,pDemo+4] = beta_CountyTime[i]; // county:time specific beta
      theta[i,2,pDemo+4] = beta_CountyTime[i]; // county:time specific beta

      theta[i,1,pDemo+5] = beta_RaceCountyTime[i]; // county:time:race specific beta
      theta[i,2,pDemo+5] = beta_RaceCountyTime[i]; // county:time:race specific beta
    }
    for (i in 1:nc){
      for (j in 1:2){
          lambda[i,j] = exp(theta[i,j] * x[i,j]);
      }
    }
  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
          C[i,j] ~ poisson_log(lambda[i,j]);
      }
    }
    beta_CountyTime ~ normal(0,SigmaCountyTime);
    beta_RaceCountyTime ~ normal(0,SigmaRaceCountyTime);
  }
#+END_SRC
** R Code
#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-poisson>>
  '
#+END_SRC

#+BEGIN_SRC R :session
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.AssaultRatio','log.BW.PopRatio')]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]

  stanprep$B.comb <- stanprep$B.uspsd + stanprep$B.wapo
  stanprep$W.comb <- stanprep$W.uspsd + stanprep$W.wapo


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cw1 = stanprep$W.uspsd,
      pDemo = 2,
      xDemo = with(stanprep,cbind(log(BW.AssaultRatio),log(BW.PopRatio))),
      xOffset = cbind(stanprep$BlackPop/1e5,stanprep$WhitePop/1e5))



  fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10)
#+END_SRC




* Simple Poisson Models
** Stan Code
#+NAME: stan-poisson
#+BEGIN_SRC stan
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cw1[nc];
    int<lower=0> pDemo; // number of demographic predictors
    vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    //  int<lower=0> p;
    vector[pDemo + 2] x[nc,2]; // nc, race, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2]; // number of shootings for county i, race j
    //  p = pDemo + 5; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1:pDemo] = xDemo[i]; // black year 1
      x[i,2,1:pDemo] = rep_vector(0,pDemo); // white year 1

      x[i,1,pDemo+1] = log(xOffset[i,1]); // black population (offset), log scale for offset
      x[i,2,pDemo+1] = log(xOffset[i,2]); // white population (offset), log scale for offset

      x[i,1,pDemo+2] = 1; // race (black)
      x[i,2,pDemo+2] = 0; // race (black)


      x[i,1,pDemo+3] = 1; // intercept
      x[i,2,pDemo+3] = 1; // intercept

      x[i,1,pDemo+4] = 1; // dummy code for county specific beta
      x[i,2,pDemo+4] = 1; // dummy code for county specific beta

      x[i,1,pDemo+5] = 1; // dummy code for county:race specific beta
      x[i,2,pDemo+5] = 0; // dummy code for county:race specific beta

    }



    for (i in 1:nc){
      C[i,1] = Cb1[i];
      C[i,2] = Cw1[i];
    }


  }
  parameters {
    row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    real beta_CountyTime[nc]; // ranfx for county:time
    real beta_RaceCountyTime[nc]; //ranfx for county:time:race
    real<lower=0> SigmaCountyTime;
    real<lower=0> SigmaRaceCountyTime;

  }
  transformed parameters {
    row_vector[pDemo+5] theta[nc,2]; // setup a theta predictor for each observation
    real<lower=0> lambda[nc,2] ; // lambda defines the poisson
    for (i in 1:nc){
      theta[i,1,1:pDemo] = beta_RaceDemo; // black year 1
      theta[i,2,1:pDemo] = beta_RaceDemo; // white year 2

      theta[i,1,pDemo+1] = 1; // population (offset)
      theta[i,2,pDemo+1] = 1; // population (offset)

      theta[i,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,pDemo+4] = beta_CountyTime[i]; // county:time specific beta
      theta[i,2,pDemo+4] = beta_CountyTime[i]; // county:time specific beta

      theta[i,1,pDemo+5] = beta_RaceCountyTime[i]; // county:time:race specific beta
      theta[i,2,pDemo+5] = beta_RaceCountyTime[i]; // county:time:race specific beta
    }
    for (i in 1:nc){
      for (j in 1:2){
          lambda[i,j] = exp(theta[i,j] * x[i,j]);
      }
    }
  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
          C[i,j] ~ poisson_log(lambda[i,j]);
      }
    }
    beta_CountyTime ~ normal(0,SigmaCountyTime);
    beta_RaceCountyTime ~ normal(0,SigmaRaceCountyTime);
  }
#+END_SRC
** R Code
#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-poisson>>
  '
#+END_SRC

#+BEGIN_SRC R :session
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.AssaultRatio','log.BW.PopRatio')]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]

  stanprep$B.comb <- stanprep$B.uspsd + stanprep$B.wapo
  stanprep$W.comb <- stanprep$W.uspsd + stanprep$W.wapo


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cw1 = stanprep$W.uspsd,
      pDemo = 2,
      xDemo = with(stanprep,cbind(log(BW.AssaultRatio),log(BW.PopRatio))),
      xOffset = cbind(stanprep$BlackPop/1e5,stanprep$WhitePop/1e5))



  fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10)
#+END_SRC





* No RandInt Stability Analysis
The following approach draws its inspiration from Generalized Linear Mixed Effects Models, but is articulated in a manner consistent with a Bayesian framework as implemented in stan.

Let $C_x$ be an observed count of shootings with associated predictors $x$. For example, $C_x$ could be the number of white people shot in Orange County, Florida, and the associated x would encode the provenance as well as demographic predictors.


$C_x \sim \text{Poisson}(\lambda_x)$

$\lambda_x = e^{\theta'x}$

$\theta$ is the vector of coefficients for the GLM. 

Let $\theta$ have block structure as 

$\theta = \begin{bmatrix} \theta_{Race:Demo} & \theta_{Offset} & \theta_{Race}  & \theta_{Race:County:Time} \end{bmatrix}$ (sequence makes indexing easier)

In most cases the elements of $\theta_{*}$ are simply one or more beta coefficients, which unless otherwise specified have uninformative priors.

Introduce an additional random variable, 

$\vec{\beta}_{County:Time}^{i} = \begin{bmatrix} \beta_{\textit{Y1, County:Time}}^i & \beta_{\textit{Y2, County:Time}}^i \end{bmatrix}$

$\vec{\beta}_{County:Time}^{i} \sim N(0,\Sigma)$

$\theta_{County:Time} = \begin{cases} \beta_{\textit{Y1, County:Time}}^i & \text{Year 1} \\ \beta_{\textit{Y2, County:Time}}^i & \text{Year 2} \end{cases}$

We are then most interested in visualizing the posterior of $\theta_{County:Time}$. The off-diagonal elements of $\Sigma$ will also tell us about the stability of the random effect over time.

Offsets
- Black Population
- White Population

Race-specific covariates
- Total Population
- log(B:W Assault Arrest Rate)
- log(B:W Population)


** Stan Code
#+NAME: stan-lme-noint
#+BEGIN_SRC stan
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cb2[nc]; 
    int<lower=0> Cw1[nc];
    int<lower=0> Cw2[nc];
    int<lower=0> pDemo; // number of demographic predictors
    vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    //  int<lower=0> p;
    vector[pDemo + 4] x[nc,2,2]; // nc, race, year, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2,2]; // number of shootings for county i, race j, time k
    //  p = pDemo + 5; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1,1:pDemo] = xDemo[i]; // black year 1
      x[i,1,2,1:pDemo] = xDemo[i]; // black year 2
      x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 1
      x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 2

      x[i,1,1,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,1,2,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,2,1,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset
      x[i,2,2,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset

      x[i,1,1,pDemo+2] = 1; // race (black)
      x[i,1,2,pDemo+2] = 1; // race (black)
      x[i,2,1,pDemo+2] = 0; // race (black)
      x[i,2,2,pDemo+2] = 0; // race (black)


      x[i,1,1,pDemo+3] = 1; // intercept
      x[i,1,2,pDemo+3] = 1; // intercept
      x[i,2,1,pDemo+3] = 1; // intercept
      x[i,2,2,pDemo+3] = 1; // intercept

      // x[i,1,1,pDemo+4] = 1; // dummy code for county:time specific beta
      // x[i,1,2,pDemo+4] = 1; // dummy code for county:time specific beta
      // x[i,2,1,pDemo+4] = 1; // dummy code for county:time specific beta
      // x[i,2,2,pDemo+4] = 1; // dummy code for county:time specific beta

      x[i,1,1,pDemo+4] = 1; // dummy code for county:time:race specific beta
      x[i,1,2,pDemo+4] = 1; // dummy code for county:time:race specific beta
      x[i,2,1,pDemo+4] = 0; // dummy code for county:time:race specific beta
      x[i,2,2,pDemo+4] = 0; // dummy code for county:time:race specific beta

    }



    for (i in 1:nc){
      C[i,1,1] = Cb1[i];
      C[i,1,2] = Cb2[i];
      C[i,2,1] = Cw1[i];
      C[i,2,2] = Cw2[i];
    }


  }
  parameters {
    row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    //  row_vector[2] beta_CountyTime[nc]; // ranfx for county:time
    row_vector[2] beta_RaceCountyTime[nc]; //ranfx for county:time:race
    corr_matrix[2] SigmaCountyTime; // covar for county:time
    corr_matrix[2] SigmaRaceCountyTime; // covar for county:time:race



  }
  transformed parameters {
    real<lower=0> lambda[nc,2,2] ; // lambda defines the poisson
    row_vector[pDemo+4] theta[nc,2,2]; // setup a theta predictor for each observation
    for (i in 1:nc){
      theta[i,1,1,1:pDemo] = beta_RaceDemo; // black year 1
      theta[i,1,2,1:pDemo] = beta_RaceDemo; // black year 2
      theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 1
      theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 2

      theta[i,1,1,pDemo+1] = 1; // population (offset)
      theta[i,1,2,pDemo+1] = 1; // population (offset)
      theta[i,2,1,pDemo+1] = 1; // population (offset)
      theta[i,2,2,pDemo+1] = 1; // population (offset)

      theta[i,1,1,pDemo+2] = beta_Race; // race (black)
      theta[i,1,2,pDemo+2] = beta_Race; // race (black)
      theta[i,2,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,1,2,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,2,pDemo+3] = beta_Int; // intercept (global)

      // theta[i,1,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      // theta[i,1,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta
      // theta[i,2,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      // theta[i,2,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta

      theta[i,1,1,pDemo+4] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,1,2,pDemo+4] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
      theta[i,2,1,pDemo+4] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,2,2,pDemo+4] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
    }
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          lambda[i,j,k] = exp(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          C[i,j,k] ~ poisson_log(lambda[i,j,k]);
        }
      }
    }
  //  beta_CountyTime ~ multi_normal(rep_row_vector(0,2),SigmaCountyTime);
    beta_RaceCountyTime ~ multi_normal(rep_row_vector(0,2),SigmaRaceCountyTime);
    }
#+END_SRC
** R Code
#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-lme-noint>>
  '
#+END_SRC



#+BEGIN_SRC R :session
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.AssaultRatio','log.BW.PopRatio')]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cb2 = stanprep$B.wapo,
      Cw1 = stanprep$W.uspsd,
      Cw2 = stanprep$W.wapo,
      pDemo = 2,
      xDemo = with(stanprep,cbind(log(BW.AssaultRatio),log(BW.PopRatio))),
      xOffset = cbind(stanprep$BlackPop/1e5,stanprep$WhitePop/1e5)
      )




  fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10,verbose=T)
#+END_SRC

* Temporal Stability Analysis
The following approach draws its inspiration from Generalized Linear Mixed Effects Models, but is articulated in a manner consistent with a Bayesian framework as implemented in stan.

Let $C_x$ be an observed count of shootings with associated predictors $x$. For example, $C_x$ could be the number of white people shot in Orange County, Florida, and the associated x would encode the provenance as well as demographic predictors.


$C_x \sim \text{Poisson}(\lambda_x)$

$\lambda_x = e^{\theta'x}$

$\theta$ is the vector of coefficients for the GLM. 

Let $\theta$ have block structure as 

$\theta = \begin{bmatrix} \theta_{Race:Demo} & \theta_{Offset} & \theta_{Race}  & \theta_{Race:County:Time} \end{bmatrix}$ (sequence makes indexing easier)

In most cases the elements of $\theta_{*}$ are simply one or more beta coefficients, which unless otherwise specified have uninformative priors.

Introduce an additional random variable, 

$\vec{\beta}_{County:Time}^{i} = \begin{bmatrix} \beta_{\textit{Y1, County:Time}}^i & \beta_{\textit{Y2, County:Time}}^i \end{bmatrix}$

$\vec{\beta}_{County:Time}^{i} \sim N(0,\Sigma)$

$\theta_{County:Time} = \begin{cases} \beta_{\textit{Y1, County:Time}}^i & \text{Year 1} \\ \beta_{\textit{Y2, County:Time}}^i & \text{Year 2} \end{cases}$

We are then most interested in visualizing the posterior of $\theta_{County:Time}$. The off-diagonal elements of $\Sigma$ will also tell us about the stability of the random effect over time.

Offsets
- Black Population
- White Population

Race-specific covariates
- Total Population
- log(B:W Assault Arrest Rate)
- log(B:W Population)


** Stan Code
#+NAME: stan-lme
#+BEGIN_SRC stan
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cb2[nc]; 
    int<lower=0> Cw1[nc];
    int<lower=0> Cw2[nc];
    int<lower=0> pDemo; // number of demographic predictors
    vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    //  int<lower=0> p;
    vector[pDemo + 5] x[nc,2,2]; // nc, race, year, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2,2]; // number of shootings for county i, race j, time k
    //  p = pDemo + 5; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1,1:pDemo] = xDemo[i]; // black year 1
      x[i,1,2,1:pDemo] = xDemo[i]; // black year 2
      x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 1
      x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 2

      x[i,1,1,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,1,2,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,2,1,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset
      x[i,2,2,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset

      x[i,1,1,pDemo+2] = 1; // race (black)
      x[i,1,2,pDemo+2] = 1; // race (black)
      x[i,2,1,pDemo+2] = 0; // race (black)
      x[i,2,2,pDemo+2] = 0; // race (black)


      x[i,1,1,pDemo+3] = 1; // intercept
      x[i,1,2,pDemo+3] = 1; // intercept
      x[i,2,1,pDemo+3] = 1; // intercept
      x[i,2,2,pDemo+3] = 1; // intercept

      x[i,1,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,1,2,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,2,pDemo+4] = 1; // dummy code for county:time specific beta

      x[i,1,1,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,1,2,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,2,1,pDemo+5] = 0; // dummy code for county:time:race specific beta
      x[i,2,2,pDemo+5] = 0; // dummy code for county:time:race specific beta

    }



    for (i in 1:nc){
      C[i,1,1] = Cb1[i];
      C[i,1,2] = Cb2[i];
      C[i,2,1] = Cw1[i];
      C[i,2,2] = Cw2[i];
    }


  }
  parameters {
    row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    row_vector[2] beta_CountyTime[nc]; // ranfx for county:time
    row_vector[2] beta_RaceCountyTime[nc]; //ranfx for county:time:race
    corr_matrix[2] SigmaCountyTime; // covar for county:time
    corr_matrix[2] SigmaRaceCountyTime; // covar for county:time:race



  }
  transformed parameters {
    real<lower=0> lambda[nc,2,2] ; // lambda defines the poisson
    row_vector[pDemo+5] theta[nc,2,2]; // setup a theta predictor for each observation
    for (i in 1:nc){
      theta[i,1,1,1:pDemo] = beta_RaceDemo; // black year 1
      theta[i,1,2,1:pDemo] = beta_RaceDemo; // black year 2
      theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 1
      theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 2

      theta[i,1,1,pDemo+1] = 1; // population (offset)
      theta[i,1,2,pDemo+1] = 1; // population (offset)
      theta[i,2,1,pDemo+1] = 1; // population (offset)
      theta[i,2,2,pDemo+1] = 1; // population (offset)

      theta[i,1,1,pDemo+2] = beta_Race; // race (black)
      theta[i,1,2,pDemo+2] = beta_Race; // race (black)
      theta[i,2,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,1,2,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,1,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta
      theta[i,2,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,2,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta

      theta[i,1,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,1,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
      theta[i,2,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,2,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
    }
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          lambda[i,j,k] = exp(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          C[i,j,k] ~ poisson_log(lambda[i,j,k]);
        }
      }
    }
    beta_CountyTime ~ multi_normal(rep_row_vector(0,2),SigmaCountyTime);
    beta_RaceCountyTime ~ multi_normal(rep_row_vector(0,2),SigmaRaceCountyTime);
    }
#+END_SRC
** R Code
#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-lme>>
  '
#+END_SRC



#+BEGIN_SRC R :session
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.AssaultRatio','log.BW.PopRatio')]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cb2 = stanprep$B.wapo,
      Cw1 = stanprep$W.uspsd,
      Cw2 = stanprep$W.wapo,
      pDemo = 2,
      xDemo = with(stanprep,cbind(log(BW.AssaultRatio),log(BW.PopRatio))),
      xOffset = cbind(stanprep$BlackPop/1e5,stanprep$WhitePop/1e5)
      )




  fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10,verbose=T)
#+END_SRC
* Old
** Modeling
*** Poisson

**** Notes
This approach came out of a meeting with Kerby Shedden on [2017-08-04 Fri].

Let's make this super simple and assume that we have just one county and aren't concerned with race.

X is observed shootings in a time period, so $X \sim \text{Poisson}(\lambda)$
We could say that $\lambda \sim N(\mu,\sigma)$

Let's introduce the notion of repeated measures, so now

$X = (X_{1},X_{2})$

now, $X_{i} \sim Poisson(\lambda_{i})$

$\lambda_{i} \sim \text{MvNorm}(\mu,\Sigma)$

We're interested in inference on $\Sigma$, particularly the off-diagonal elements, as these tell us about the consistency of the risk.

Now let's consider that different counties will have different populations.

We define the interval space to be people, so $\lambda$ is actually the shooting rate per capita.
Even narrower: the rate is shootings per N*person-year where N is some scaling term.

Now, $X_i \sim N_{i}*Poisson(\lambda_i)$

let 
- X_{i,j} be the number of observed shootings at timeperiod i for county j
- N_i,j be the number of people (in convenient units) at timeperiod i for county j

$X_{i,j} \sim N_{i,j} * Poisson(\lambda_i,j)$

and $\lambda_{i,j} \sim \text{MNormal}(\mu_{j},\Sigma)$

$\mu_j$ = linear model based on covariates for that county at time i

$\Sigma \sim$ some prior? I dunno


Kerby's notation looks like Poisson regression

$E(Y|x) = e^{\theta'x}$ yeah definitely
**** Formal Model: One Year, No Covariates
$r_{i} \sim Pois(\lambda_{i})$: Rate (Shootings/PersonYear) in County i

Flat prior for $\lambda$
**** Formal Model: One Year, with Covariates
$r_{i} \sim Pois(\lambda_{i})$: Rate (Shootings/PersonYear) in County i

$\lambda_i = \beta X_i$

Flat prior for $\lambda$
***** Code
****** Stan
#+NAME: stan-oneyear-cov
#+BEGIN_SRC stan

#+END_SRC

**** Formal Model: no Covariates
$r_{i,t} \sim Pois(\lambda_{i,t})$: Rate (Shootings/PersonYear) in County i during year t

$\vec{\lambda_i} = \begin{bmatrix} \lambda_{i,1}&\lambda_{i,2} & \ldots & \lambda_{i,T} \end{bmatrix} =  N(\vec{\mu_i},\Sigma_i)$

$\vec{\mu_i} = \begin{bmatrix} \mu_{i,1}&\mu_{i,2} & \ldots & \mu_{i,T} \end{bmatrix}$

Flat prior on $\mu_{i,t}$
**** Poisson Regression: why exponentiate?
This started as a draft question on CrossValidated, but perhaps I'm starting to answer my own question as so often happens with Stack Overflow

I'm new to Poisson Regression and am trying to understand the motivation behind what is often either treated definitionally or as an assumption in most texts I can find on the matter.

Specifically, in Poisson regression, there is the assumption that 
$E[Y \mid x] = e^{\theta'x}$

In other words, this makes sense if one has the expectation that one is observing data generated under a Poisson distribution whose rate parameter $\lambda$ is parameterized as the exponentation of some linear function of the predictors.

/Why/ is this a reasonable expectation? Is it simply the case that this parameterization gives nice properties to the surface of the likelihood that will be maximized (like convexity) or is it that the case that in the motivating applications for the original development of Poisson regression this was a reasonable assumption.

Perhaps the motivation is more easily found in the motivation for the Proportional hazards motivation.

As an example, suppose that I'm looking at some discrete count phenomenon, like the number of people who come to the emergency room complaining of respiratory difficulty. For every one increase in the pollution in the air, let's assume that the rate of admittance shuold also increase by one unit.

On the other hand, let's think about what happens in the exponential case. Let's start with 0 pollution. The rate is $\lambda = \lambda_0 e^{0}$. Now the pollution rate increases by one unit. $\lambda = \lambda_0 e^{1}$. So the rate has increased by e. Now pollution increases one more unit. $\lambda = \lambda_0 e^2$. Every unit increase in the predictor will yield an *exponential* increase in the hazard. 

Presumably, there's a cool proportionality that falls out of here somewhere that ultimately might be at the crux for the type of estimation that I want to do.

The particularly cool thing here falls out when we're only really interested in estimating a /relative/ risk, which is precisely what we're after here. Let's suppose that we care about how the risk changes when we go from white (x=0) to black (x=1).

Risk for white is $\lambda = \lambda_0 e^0$
Risk for black is $\lambda = \lambda_0 e^{\beta}$

Let's further assume that we would be hard pressed to directly estimate $\lambda_0$

If we care about the relative risk, then we have $RR = \frac{\text{Risk-Black}}{\text{Risk-White}} =  \frac{\lambda_0 e^\beta}{\lambda_0 e^0} = e^\beta$
thus we have totally avoided having to estimate $\lambda_0$. The key, then, is in the formulation. We basically have counterfactual observations, because each county is "observed" twice insofar as it provides two scalar counts.





**** Formal Model: with Covariates

Let $s_{i,t}$ be the number of people shot in county $i$ in year $t$.

We assume that 

$s_{i,t} \sim Pois(\lambda_{i,t})$


$\vec{\lambda_i} = \begin{bmatrix} \lambda_{i,1}&\lambda_{i,2} & \ldots & \lambda_{i,T} \end{bmatrix} \sim N(\vec{\mu_i},\Sigma_i)$


$\vec{\mu_i} = \begin{bmatrix} \mu_{i,1}&\mu_{i,2} & \ldots & \mu_{i,T} \end{bmatrix}$

$\mu_{i,t} = \vec{\beta} X_{i,t}$ alternative version has $\mu$ drawn from a distribution

or, for Poisson-regression like approach

$\mu_{i,t} = e^{\vec{\beta} X_{i,t}}$

where $\vec{\beta}$ are coefficients linking the $K$ predictors in $X_{i,t}$ to the rate parameter $\lambda$

$\vec{\beta} =  \begin{bmatrix} \beta_{1}&\beta_{2} & \ldots & \beta_{K} \end{bmatrix}$

$\beta_k \sim \text{Cauchy}(0,5)$ this decision is random, could easily be flat improper prior

By examining the posterior of the off-diagonal elements of $\Sigma_i$ we can explore the degree to which counties consistently over- or under-perform relative to their expectation in 



**** Stan Implementation
***** Stan Code
#+NAME: somestancode
#+BEGIN_SRC stan :noweb yes
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> X[nc][2]; // number of shootings for county i at time j
  }

  parameters {
    real<lower=0> lambda; // the shooting rate?
  }

  model {
    for (i in 1:nc)
        X[i] ~ normal(mu,sigma);
  }
#+END_SRC
***** R Code
#+BEGIN_SRC R :noweb yes
  mycode <- " 
  <<somestancode>> 
  "
#+END_SRC

#+RESULTS:
|                                                                      |
| data {                                                               |
| int<lower=0> nc ; // number of counties                              |
| int<lower=0> X[nc][2]; // number of shootings for county i at time j |
| }                                                                    |
|                                                                      |
| parameters {                                                         |
| real<lower=0> lambda; // the shooting rate?                          |
| }                                                                    |
|                                                                      |
| model {                                                              |
| for (i in 1:nc)                                                      |
| X[i] ~ normal(mu,sigma);                                             |
| }                                                                    |
|                                                                      |

**** Example Poisson Regression (Vanilla R)
#+BEGIN_SRC R :session
  library(rstan)
  library(ISwR)

  data(eba1977)
  summary(eba1977)

  glm1 <- glm(formula     = cases ~ age + city + offset(log(pop)),
              family      = poisson(link = "log"),
              data        = eba1977)
  summary(glm1)
#+END_SRC

#+RESULTS:
**** Formal Model: Ratios

$C^B_{i,t}$ is the count of black people shot in county i in year t


$C^W_{i,t}$ is the count of white people shot in county i in year t


$C^B_{i,t} \sim \text{Pois}(\lambda^B_{i,t})$ 

$C^W_{i,t} \sim \text{Pois}(\lambda^W_{i,t})$


$\vec{C}_{i,t} = \begin{bmatrix} C^B_{i,t} & C^W_{i,t} \end{bmatrix}$ put counts into time vector

$R_{i,t} = \frac{\lambda^B_{i,t}}{\lambda^W_{i,t}}$ is the relative risk of being shot in county i in year t

$\vec{R}_{i} = \begin{bmatrix} R_{i,1} & R_{i,2} & \dots & R_{i,T} \end{bmatrix}$ put R in time vector

$\vec{R}_{i} \sim \text{N}(\vec{\mu}_i,\Sigma)$ 

$\vec{\mu}_i = \begin{bmatrix} \mu_{i,1} & \mu_{i,2} & \dots & \mu_{i,T} \end{bmatrix}$ put mu in time vector

$\mu_{i,t} = e^{x_{i,t}\beta}$ or equivalently $ln(\mu_{i,t}) = x_{i,t}\beta$

where $x_{i,t}$ is a vector of predictors for county i at time t (e.g. population, relative arrest rate, etc)

Sentence one.
Sentence two.

Paragraph two.
Sentence 2-2.





** STAN Experimentation
*** Seemingly Unrelated Regressions
SUR is covered on page 77. I think this is precisely our case. We want to know if the regressions are indeed related. 


$y_n = \beta x_n + \epsilon_n$ 

$\epsilon_n \sim \mathbb{N}(0,\Sigma)$

$y_n$ K-vector of observations for n'th county

$\beta$: $(K x J)$ matrix of coefficients

$x_n$: J-vector (as a row-vector) of J predictors for nth county

$\epsilon_n$ is K-vector of errors for n'th "county"
*** Poisson Regression (Univariate)

$y_n \sim \text{Pois}(\lambda_n)$

$\lambda_n = e^{x_n\beta}$

this seems freakishly simple... and is error-less
*** Negative Binomial Regression
I think that this is *actually* what Kerby drew. It's like Poisson regression, but with an extra dispersion parameter, usually theta.


*** Seemingly Unrelated Poisson Regressions
Presume that a stochastic process gives rise to a latent variable $\lambda_{n,t}$ for each county at each time t.





$\lambda_n = \beta x_n + \epsilon_n$

we'll need to introduce a latent variable z_n, which I guess is like lambda?

$y_{n,t} = 

$y_{n,t} \sim \text{Pois}(\lambda_{n,t})$
*** Seemingly Unrelated Poisson Regressions of Rates (with Offsets)



*** Our Setting: one time point, but with poisson, one stage model
Each county should likely get identical coefficients, with unique predictors.

In a sense, we want something more like a random effect for each county.

$y_n \sim \text{Pois}(x_n\beta)$ shootings in county n are drawn from Poisson.

$y_n \sim \text{Pois}(\lambda_n)$ shootings in county n are drawn from Poisson with unique lambda

$\lambda_n \sim N(\beta x_n,\sigma)$ The lambda for each county is drawn from a normal, whose central tendency is determined by the prediction. If counties are generally dissimilar, $\sigma$ will be high. If counties generally behave as predicted, $\sigma$ will be small.

If we just expand the scope a tiny bit, we can fold in time

$y_{n,t} \sim \text{Pois}(\lambda_{n,t})$ shootings in county n for time t are drawn from Poisson

$\vec{\lambda_n} = \begin{bmatrix} \lambda_{n,1}&\lambda_{n,2} & \ldots & \lambda_{n,T} \end{bmatrix} \sim N(\vec{\mu_n},\Sigma_n)$ ($\vec{\mu_n}$) is just a convenience here

$\vec{\mu_n} = \begin{bmatrix} x_{n,1}\beta & x_{n,2}\beta & \ldots & x_{n,T}\beta \end{bmatrix}$

*** Hierarchical Linear Regression

$y_n \sim N(x_n\beta,\sigma)$
$\beta_k \sim N(0,\tau)$
$\tau \sim \text{Cauchy}(0,2.5)$

As $\tau \to 0$ and $\beta_k \to 0$, the posterior density
$p(\beta,\tau,\sigma \mid y,x) \propto p(y \mid x,\beta,\tau,\sigma)$ 
grwqos without bound.

*** Hierarchical Logistic Regression
This is worked out beginning on page 62 of the stan manual.

Let's make this explicit. Our outcome is catching a cold. We are tracking a bunch of people who come from different counties.

Each person gets sick or not, $y_n \in \{0,1\}$ and that person comes from one county, indicated by $ll_n \in \{1,\ldots,L\}$. Each person also has a predictor vector $x_n \in \mathbb{R}^D$. Let's say D = 3, and the vector includes 1) their age; 2) if they are male; and 3) if they wash their hands regularly. Each county will get its own coefficient vector relating the predictor vector to risk, $\beta_{l} \in \mathbb{R}^D$. We impose hierarchy in that we draw each individual coefficient for each county, $\beta_{l,d} \in \mathbb{R}$ from a prior that is estimated with the data. The prior, once estimated, determines the amount of pool. If each county acts very similarly, there will be strong pooling enforced by low hierarchical variance. If counties behave very differently, the weak pooling will be enforced by high hierarchical variance.

Here's my attempt to write this as a formal model based on the stan-code below. Let's not try to twist it to fit the police shooting example, because we have a sort of degenerate hierarchical model, as if we've just measured one super person from each county.

$y_n \sim \text{bernoulli}(\frac{1}{1 + e^{-\beta_l X_n}})$ level of the observation

$\beta_{l,d} \sim N(\mu_d,\sigma_d)$ county-level

$\mu_d \sim N(0,100)$ (this is a hyperprior, I suppose?)



**** Stan
#+NAME: stan-hlr
#+BEGIN_SRC stan
  data {
    int<lower=1> D; // this is the number of predictors
    int<lower=0> N; // number of observations
    int<lower=1> L; // number of counties
    int<lower=0,upper=1> y[N]; // observations (did peeps get sick?)
    int<lower=1,upper=L> ll[N]; // county assignments for the peeps
    row_vector[D} x[N]; //design vector, one for each peep
  }
  parameters {
    real mu[D]; // dunno what this is
    real<lower=0> sigma[D]; // hierarchical variance?
    vector[D] beta[L]; // D-length vector for each county l
  }
  model {
    for (d in 1:D) { // consider each predictor separately
      mu[d] ~ normal(0,100); // mu is the mean from which beta is drawn, same for all counties
      for (l in 1:L) {
        beta[l,d] ~ normal(mu[d],sigma[d]); // each beta for each county is drawn from a prior
      }
    }
    for (n in 1:N) {
      y[n] ~ bernoulli(inv_logit(x[n] * beta[ll[n]])); // each observation is a bernoulli draw with probability p defined by the inverse logit of dot product of predictor and coefficients (standard logistic regression)
  }


#+END_SRC
*** Double Specification
**** Stan
#+NAME: stan-double
#+BEGIN_SRC stan
  data {
    int<lower=0> N;
    vector[N] x;
    vector[N] y;
  }

  parameters {
    real alpha;
    real beta;
    real<lower=0> sigma;
  }
  model {
    y ~ normal(alpha + beta * x, sigma);

    y ~ normal(100,0.1);
  }

#+END_SRC
**** R Setup
#+BEGIN_SRC R :session :noweb yes :results none
  library(rstan)


  N <- 100

  sigma <- 2
  beta <- 3

  noise <- rnorm(N,mean=0,sd=sigma)

  x <- rnorm(N)

  y <- x*beta + noise

  model.data <- list(N,x,y)

  model.stan <- '
  <<stan-double>>
  '


#+END_SRC


**** Run Model
#+BEGIN_SRC R :session 

  fit <- stan(model_code = model.stan,data=model.data,iter=300)

  summary(fit)

  plot(fit)

  print(fit)
#+END_SRC
