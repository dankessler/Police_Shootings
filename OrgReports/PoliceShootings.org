* Introduction
** The Problem
The US has recently witnessed a number of high-profile deaths of African-Americans at the hands of the police
- Michael Brown
- Eric Garner
- Walter Scott

There is a pressing need to study this phenomenon quantitatively. Why?
- Case reports are insufficient and could even be misleading
- Better understand the causes
- Help shape solutions
** An Important Step Forward
** Key Results
** Present work
Main Questions
1. Are Ross's main results replicated in WashPo database?
2. Is there temporal stability in spatial variation in risk ratios?
* Data Processing: 2 Datasets
** Goal
Our goal is to arrive at a dataset where each row encodes one county. We will have four outcome measures per county (Period X Race). We also require the following covariates
- Black Population (for use as offset)
- White Population (for use as offset)
- Total Population
- B:W Population
- B:W Arrest Rate for Assault

We also have a dataset of county-level covariates, obtained from Dr. Ross's github repository. This isn't provided in raw format, but is rather the result of merging county-level shooting rates with some unprovided demographic file. Here, I extract only the demographics of interest.
** Load Source Data Files

#+BEGIN_SRC R :session :results none :export code
  cases.uspsd <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/U.S. Police Shootings Data (Cleaned).csv')

  cases.wapo <- read.csv('../submodules/data-police-shootings/fatal-police-shootings-data.csv')

  mapfile <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/MapFileData-WithCountyResultsAndCovariates.csv')

  mapfile$BW.PopRatio <- mapfile$BAC_TOT / mapfile$WA_TOT
  mapfile$BlackAssaultArrest <- mapfile$AssaultsBlack.sum / mapfile$BAC_TOT
  mapfile$WhiteAssaultArrest <- mapfile$AssaultsWhite.sum / mapfile$WA_TOT
  mapfile$BW.AssaultRatio <- mapfile$BlackAssaultArrest / mapfile$WhiteAssaultArrest

  county.demographics <- data.frame(
      County.FIPS.Code = mapfile$County.FIPS.Code,
      County.Name = mapfile$County.Name,
      State = mapfile$State,
      BlackPop = mapfile$BAC_TOT,
      WhitePop = mapfile$WA_TOT,
      TotPop = mapfile$TOT_POP,
      BW.PopRatio = mapfile$BW.PopRatio,
      BW.AssaultRatio = mapfile$BW.AssaultRatio)

  rm(mapfile) # tidy up the namespace
#+END_SRC

** Map to Counties
*** USPSD
We can skip this step, because Cody Ross's github already provides data in a more processed format that we'll load downstream.

*** WaPo
The Washington Post Database has info at a city-state level. In order to merge the WaPo database with the USPSD database, we must annotate each entry with the county and associated code.

First, let's define some helpful functions. We rely on the geo.lookup function from the acs package. However, it sometimes returns multiple (or no) county mappings for certain entities, which we must correct by hand using a hand-tuned map.  In an attempt to make future City, State -> County mapping, we've made the hand-tuned map available on [[https://github.com/dankessler/city_county_map][github]].

#+BEGIN_SRC R :session :results none :export code
  library(acs)

  addCounties <- function(df){
      names(df)[grepl('state',names(df))] <- 'State' # shift case to match Cody
      cities <- unique(df[,c('city','State')])
      handmap <- read.csv('/home/kesslerd/repos/Analysis/PoliceShootings/city_county_map/HandMappings.csv')
      cities$County.Name <- mapply(getCounty,cities$State,cities$city,MoreArgs=list(handmap=handmap))
      cities$County.Name <- unlist(cities$County.Name)
      return(merge(df,cities,all.x=TRUE))
  }

  getCounty <- function(state,city,handmap){
      hand.candidate <- handmap[handmap$city==city & handmap$State==state,]
      if (nrow(hand.candidate)==1){
          return(hand.candidate['Hand_CountyName'])
      }

      candidates <- geo.lookup(state=state,place=city)
      if (nrow(candidates) < 2){ # confirm that we have some hits
          return ('NoMatch')
      }
      candidates <- candidates[-1,] # drop the first hit which is null
      candidates <- unique(candidates) # deal with duplicates

      dists <- adist(city,candidates[,'place.name'])
      shortest <- min(dists)
      dups <- sum(dists==shortest)
      if (dups>1){
          return('MultiRowMatch')
      }
      bestind <- which.min(dists)
      county <- candidates[bestind,'county.name']
      return(county)
  }
#+END_SRC

Next, we'll use them to clean up the WaPo dataset.

#+BEGIN_SRC R :session :results none :export code
  cases.wapo <- addCounties(cases.wapo)
#+END_SRC

** Summarize by County, Armed Status, and Race
Again, the treatment for each file is a bit different, as they structure their incident data differently.
*** USPSD
Conveniently, Cody's repository provides the file in a format with summaries already calculated and covariates included. For convenience sake, we're going to jump to this point in the stream and extract only the critical pieces of information, so that we can merge with WaPo data and add covariates later.

#+BEGIN_SRC R :session :results none :exports code
  cases.uspsd <- read.csv('../submodules/ctross_publications/PLOS-RacialBiasPoliceShootings/Data/MaintainedImprovedData/MapFileData-WithCountyResultsAndCovariates.csv')

  uspsd <- data.frame(
      State = cases.uspsd$State,
      County.Name = cases.uspsd$County.Name,
      B = cases.uspsd$BlackUnarmed,
      W = cases.uspsd$WhiteUnarmed)

  ## uspsd <- data.frame(
  ##     State = cases.uspsd$State,
  ##     County.Name = cases.uspsd$County.Name,
  ##     B = cases.uspsd$BlackUnarmed + cases.uspsd$BlackArmed,
  ##     W = cases.uspsd$WhiteUnarmed + cases.uspsd$WhiteArmed)
#+END_SRC
*** WaPo
The WaPo data is quite granular with respect to what weapon (if any) was carried by the civilian. Because we are interested specifically in unarmed civilians, we only count cases annotated as "unarmed."
#+BEGIN_SRC R :session :results none :exports code
  library(reshape2)

  wapo <- dcast(cases.wapo, State + County.Name ~ race, subset = .(race %in% c('B','W') & cases.wapo$armed=='unarmed'),fun.aggregate=length)

  #wapo <- dcast(cases.wapo, State + County.Name ~ race, subset = .(race %in% c('B','W')),fun.aggregate=length)
#+END_SRC
** Merge USPSD and WaPo
Next we combine the two files, with suffixes such that we can identify the source. There are a small number of counties that appear in only one dataset. In these cases, we replace the missing data with 0 shootings, since this means there were none reported.

#+BEGIN_SRC R :session :exports code :results none
  unarmed.counts <- merge(wapo,uspsd,by=c('State','County.Name'),suffixes=c('.wapo','.uspsd'))

  # change NAs to 0s
  unarmed.counts[,c('B.wapo','W.wapo','B.uspsd','W.uspsd')] <- apply(unarmed.counts[,c('B.wapo','W.wapo','B.uspsd','W.uspsd')],c(1,2),function(x){ifelse(is.na(x),0,x)})
#+END_SRC
** Merge with County-Level Covariates
For all of the counties that appear in our merged file, we add county level demographics.

#+BEGIN_SRC R :session :exports code :results none
  unarmed.counts <- merge(unarmed.counts,county.demographics,all.x=TRUE,all.y=FALSE)
#+END_SRC
* Temporal Stability Analysis
** Math
The following approach draws its inspiration from Generalized Linear Mixed Effects Models, but is articulated in a manner consistent with a Bayesian framework as implemented in stan.

Let $C_x$ be an observed count of shootings with associated predictors $x$. For example, $C_x$ could be the number of white people shot in Orange County, Florida, and the associated x would encode the provenance as well as demographic predictors.


$C_x \sim \text{Poisson}(\lambda_x)$

$\lambda_x = e^{\theta'x}$

$\theta$ is the vector of coefficients for the GLM. 

Let $\theta$ have block structure as 

$\theta = \begin{bmatrix} \theta_{Race:Demo} & \theta_{Offset} & \theta_{Race}  & \theta_{County:Time} & \theta_{Race:County:Time} \end{bmatrix}$

In most cases the elements of $\theta_{*}$ are simply one or more beta coefficients, which unless otherwise specified have uninformative priors.

Introduce two additional random variable, 

$\vec{\beta}_{County:Time}^{i} = \begin{bmatrix} \beta_{\textit{D1, County:Time}}^i & \beta_{\textit{D2, County:Time}}^i \end{bmatrix}$

$\vec{\beta}_{Race:County:Time}^{i} = \begin{bmatrix} \beta_{\textit{YD, Race:County:Time}}^i & \beta_{\textit{D2, Race:County:Time}}^i \end{bmatrix}$

$\vec{\beta}_{County:Time}^{i} \sim N(0,\Sigma)$

$\theta_{County:Time} = \begin{cases} \beta_{\textit{D1, County:Time}}^i & \text{Year 1} \\ \beta_{\textit{D2, County:Time}}^i & \text{Year 2} \end{cases}$

We are then most interested in visualizing the posterior of $\theta_{County:Time}$. The off-diagonal elements of $\Sigma$ will also tell us about the stability of the random effect over time.

Offsets
- Black Population
- White Population

Race-specific covariates
- Total Population
- log(B:W Assault Arrest Rate)
- log(B:W Population)
** Step 1: Shooting ~ Race | Population, Dataset
*** Stan Code
#+NAME: stan-lme1
#+BEGIN_SRC stan :eval no
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cb2[nc]; 
    int<lower=0> Cw1[nc];
    int<lower=0> Cw2[nc];
    int<lower=0> pDemo; // number of demographic predictors
    //  vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    int<lower=0> p;
    vector[pDemo + 6] x[nc,2,2]; // nc, race, year, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2,2]; // number of shootings for county i, race j, time k
    p = pDemo + 6; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      // if (pDemo>0){
      //   x[i,1,1,1:pDemo] = xDemo[i]; // black year 1
      //   x[i,1,2,1:pDemo] = xDemo[i]; // black year 2
      //   x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 1
      //   x[i,2,2,1:pDemo] = rep_vector(0,pDemo); // white year 2
      // }

      x[i,1,1,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,1,2,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,2,1,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset
      x[i,2,2,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset

      x[i,1,1,pDemo+2] = 1; // race (black)
      x[i,1,2,pDemo+2] = 1; // race (black)
      x[i,2,1,pDemo+2] = 0; // race (black)
      x[i,2,2,pDemo+2] = 0; // race (black)


      x[i,1,1,pDemo+3] = 1; // intercept
      x[i,1,2,pDemo+3] = 1; // intercept
      x[i,2,1,pDemo+3] = 1; // intercept
      x[i,2,2,pDemo+3] = 1; // intercept

      x[i,1,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,1,2,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,2,pDemo+4] = 1; // dummy code for county:time specific beta

      x[i,1,1,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,1,2,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,2,1,pDemo+5] = 0; // dummy code for county:time:race specific beta
      x[i,2,2,pDemo+5] = 0; // dummy code for county:time:race specific beta

      x[i,1,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,1,2,pDemo+6] = 1; // dummy code for dataset fx
      x[i,2,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,2,2,pDemo+6] = 1; // dummy code for dataset fx

  

    }



    for (i in 1:nc){
      C[i,1,1] = Cb1[i];
      C[i,1,2] = Cb2[i];
      C[i,2,1] = Cw1[i];
      C[i,2,2] = Cw2[i];
    }


  }
  parameters {
    //row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    real beta_Data; // effect of data source
    row_vector[2] beta_CountyTime[nc]; // ranfx for county:time
    row_vector[2] beta_RaceCountyTime[nc]; //ranfx for county:time:race
    cov_matrix[2] SigmaCountyTime; // covar for county:time
    cov_matrix[2] SigmaRaceCountyTime; // covar for county:time:race



  }
  transformed parameters {
    row_vector[pDemo+6] theta[nc,2,2]; // setup a theta predictor for each observation
    for (i in 1:nc){
      // if (pDemo>0){
      //   theta[i,1,1,1:pDemo] = beta_RaceDemo; // black year 1
      //   theta[i,1,2,1:pDemo] = beta_RaceDemo; // black year 2
      //   theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 1
      //   theta[i,2,2,1:pDemo] = beta_RaceDemo; // white year 2
      // }

      theta[i,1,1,pDemo+1] = 1; // population (offset)
      theta[i,1,2,pDemo+1] = 1; // population (offset)
      theta[i,2,1,pDemo+1] = 1; // population (offset)
      theta[i,2,2,pDemo+1] = 1; // population (offset)

      theta[i,1,1,pDemo+2] = beta_Race; // race (black)
      theta[i,1,2,pDemo+2] = beta_Race; // race (black)
      theta[i,2,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,1,2,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,1,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta
      theta[i,2,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,2,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta

      theta[i,1,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,1,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
      theta[i,2,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,2,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta

      theta[i,1,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,1,2,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,2,pDemo+6] = beta_Data; // fx of data source
    }



  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          C[i,j,k] ~ poisson_log(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
    beta_CountyTime ~ multi_normal(rep_row_vector(0,2),SigmaCountyTime);
    beta_RaceCountyTime ~ multi_normal(rep_row_vector(0,2),SigmaRaceCountyTime);
    }

  generated quantities {
    real RR[nc,2]; // relative risks (B/W) by year
    real<lower=0> lambda[nc,2,2] ; // lambda defines the poisson
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          lambda[i,j,k] = exp(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
  
    for (i in 1:nc){
      for (k in 1:2){
        RR[i,k] = lambda[i,1,k] / lambda[i,2,k];
      }
    }
  }
#+END_SRC

#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-lme1>>
  '
#+END_SRC
*** R Code

#+BEGIN_SRC R :session
  stanprep <- unarmed.counts[complete.cases(unarmed.counts),]
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  #temp <- stanprep[,c('log.BW.PopRatio'),drop=FALSE]
  #mask <- apply(temp,1,function(x){all(is.finite(x))})
  #stanprep <- stanprep[mask,]


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cb2 = stanprep$B.wapo,
      Cw1 = stanprep$W.uspsd,
      Cw2 = stanprep$W.wapo,
      pDemo = 0,
      xDemo = with(stanprep,cbind(log(TotPop),log(BW.PopRatio))),
      xOffset = cbind(stanprep$BlackPop,stanprep$WhitePop)
      )



  #fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=1000, warmup = 100, chains = 1, refresh = 10)
  fit1 <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10)

  ex1 <- extract(fit1)
#+END_SRC

** Step 2: Shooting ~ Race | Population, Dataset, B:W Ratio
*** Stan Code
#+NAME: stan-lme2
#+BEGIN_SRC stan :eval no
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cb2[nc]; 
    int<lower=0> Cw1[nc];
    int<lower=0> Cw2[nc];
    int<lower=0> pDemo; // number of demographic predictors
    real xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    int<lower=0> p;
    vector[pDemo + 6] x[nc,2,2]; // nc, race, year, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2,2]; // number of shootings for county i, race j, time k
    p = pDemo + 6; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1,1] = xDemo[i]; // black year 1
      x[i,1,2,1] = xDemo[i]; // black year 2
      x[i,2,1,1] = 0; // white year 1
      x[i,2,2,1] = 0; // white year 2


      x[i,1,1,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,1,2,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,2,1,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset
      x[i,2,2,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset

      x[i,1,1,pDemo+2] = 1; // race (black)
      x[i,1,2,pDemo+2] = 1; // race (black)
      x[i,2,1,pDemo+2] = 0; // race (black)
      x[i,2,2,pDemo+2] = 0; // race (black)


      x[i,1,1,pDemo+3] = 1; // intercept
      x[i,1,2,pDemo+3] = 1; // intercept
      x[i,2,1,pDemo+3] = 1; // intercept
      x[i,2,2,pDemo+3] = 1; // intercept

      x[i,1,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,1,2,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,2,pDemo+4] = 1; // dummy code for county:time specific beta

      x[i,1,1,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,1,2,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,2,1,pDemo+5] = 0; // dummy code for county:time:race specific beta
      x[i,2,2,pDemo+5] = 0; // dummy code for county:time:race specific beta

      x[i,1,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,1,2,pDemo+6] = 1; // dummy code for dataset fx
      x[i,2,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,2,2,pDemo+6] = 1; // dummy code for dataset fx



    }



    for (i in 1:nc){
      C[i,1,1] = Cb1[i];
      C[i,1,2] = Cb2[i];
      C[i,2,1] = Cw1[i];
      C[i,2,2] = Cw2[i];
    }


  }
  parameters {
    real beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    real beta_Data; // effect of data source
    row_vector[2] beta_CountyTime[nc]; // ranfx for county:time
    row_vector[2] beta_RaceCountyTime[nc]; //ranfx for county:time:race
    cov_matrix[2] SigmaCountyTime; // covar for county:time
    cov_matrix[2] SigmaRaceCountyTime; // covar for county:time:race



  }
  transformed parameters {
    row_vector[pDemo+6] theta[nc,2,2]; // setup a theta predictor for each observation
    for (i in 1:nc){
      theta[i,1,1,1] = beta_RaceDemo; // black year 1
      theta[i,1,2,1] = beta_RaceDemo; // black year 2
      theta[i,2,1,1] = beta_RaceDemo; // white year 1
      theta[i,2,2,1] = beta_RaceDemo; // white year 2

      theta[i,1,1,pDemo+1] = 1; // population (offset)
      theta[i,1,2,pDemo+1] = 1; // population (offset)
      theta[i,2,1,pDemo+1] = 1; // population (offset)
      theta[i,2,2,pDemo+1] = 1; // population (offset)

      theta[i,1,1,pDemo+2] = beta_Race; // race (black)
      theta[i,1,2,pDemo+2] = beta_Race; // race (black)
      theta[i,2,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,1,2,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,1,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta
      theta[i,2,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,2,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta

      theta[i,1,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,1,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
      theta[i,2,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,2,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta

      theta[i,1,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,1,2,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,2,pDemo+6] = beta_Data; // fx of data source
    }



  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          C[i,j,k] ~ poisson_log(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
    beta_CountyTime ~ multi_normal(rep_row_vector(0,2),SigmaCountyTime);
    beta_RaceCountyTime ~ multi_normal(rep_row_vector(0,2),SigmaRaceCountyTime);
    }

  generated quantities {
    real RR[nc,2]; // relative risks (B/W) by year
    real<lower=0> lambda[nc,2,2] ; // lambda defines the poisson
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          lambda[i,j,k] = exp(theta[i,j,k] * x[i,j,k]);
        }
      }
    }

    for (i in 1:nc){
      for (k in 1:2){
        RR[i,k] = lambda[i,1,k] / lambda[i,2,k];
      }
    }
  }
#+END_SRC

#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-lme2>>
  '
#+END_SRC

*** R Code

#+BEGIN_SRC R :session
  stanprep <- unarmed.counts[complete.cases(unarmed.counts),]
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.PopRatio'),drop=FALSE]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cb2 = stanprep$B.wapo,
      Cw1 = stanprep$W.uspsd,
      Cw2 = stanprep$W.wapo,
      pDemo = 1,
      xDemo = log(stanprep$BW.PopRatio),
      xOffset = cbind(stanprep$BlackPop,stanprep$WhitePop)
  )




  #fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=1000, warmup = 100, chains = 1, refresh = 10)
  fit2 <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10)

  ex2 <- extract(fit2)
#+END_SRC

** Step 3: Shooting ~ Race | Population, Dataset, B:W Ratio, Arrest Rate Ratio
*** Stan Code
#+NAME: stan-lme3
#+BEGIN_SRC stan :eval no
  data {
    int<lower=0> nc ; // number of counties
    int<lower=0> Cb1[nc];
    int<lower=0> Cb2[nc]; 
    int<lower=0> Cw1[nc];
    int<lower=0> Cw2[nc];
    int<lower=0> pDemo; // number of demographic predictors
    vector[pDemo] xDemo[nc]; // hold demographic predictors
    real xOffset[nc,2]; // offset predictor (for each subpopulation)
  }
  transformed data {
    int<lower=0> p;
    vector[pDemo + 6] x[nc,2,2]; // nc, race, year, predictor (+1 for intercept, in last)
    int<lower=0> C[nc,2,2]; // number of shootings for county i, race j, time k
    p = pDemo + 6; // number of predictors (demographics + offset + race + int + ranint + ranslope)
    for (i in 1:nc){
      x[i,1,1,1:pDemo] = xDemo[i]; // black year 1
      x[i,1,2,1:pDemo] = xDemo[i]; // black year 2
      x[i,2,1,1:pDemo] = rep_vector(0,pDemo); // white year 1
      x[i,2,2,1:pDemo] = rep_vector(0,pDemo); // white year 2


      x[i,1,1,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,1,2,pDemo+1] = log(xOffset[i,1]); // population (offset), log scale for offset
      x[i,2,1,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset
      x[i,2,2,pDemo+1] = log(xOffset[i,2]); // population (offset), log scale for offset

      x[i,1,1,pDemo+2] = 1; // race (black)
      x[i,1,2,pDemo+2] = 1; // race (black)
      x[i,2,1,pDemo+2] = 0; // race (black)
      x[i,2,2,pDemo+2] = 0; // race (black)


      x[i,1,1,pDemo+3] = 1; // intercept
      x[i,1,2,pDemo+3] = 1; // intercept
      x[i,2,1,pDemo+3] = 1; // intercept
      x[i,2,2,pDemo+3] = 1; // intercept

      x[i,1,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,1,2,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,1,pDemo+4] = 1; // dummy code for county:time specific beta
      x[i,2,2,pDemo+4] = 1; // dummy code for county:time specific beta

      x[i,1,1,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,1,2,pDemo+5] = 1; // dummy code for county:time:race specific beta
      x[i,2,1,pDemo+5] = 0; // dummy code for county:time:race specific beta
      x[i,2,2,pDemo+5] = 0; // dummy code for county:time:race specific beta

      x[i,1,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,1,2,pDemo+6] = 1; // dummy code for dataset fx
      x[i,2,1,pDemo+6] = 0; // dummy code for dataset fx
      x[i,2,2,pDemo+6] = 1; // dummy code for dataset fx



    }



    for (i in 1:nc){
      C[i,1,1] = Cb1[i];
      C[i,1,2] = Cb2[i];
      C[i,2,1] = Cw1[i];
      C[i,2,2] = Cw2[i];
    }


  }
  parameters {
    row_vector[pDemo] beta_RaceDemo; // race:demographic interaction betas
    real beta_Int; // intercept
    real beta_Race; // race fx
    real beta_Data; // effect of data source
    row_vector[2] beta_CountyTime[nc]; // ranfx for county:time
    row_vector[2] beta_RaceCountyTime[nc]; //ranfx for county:time:race
    cov_matrix[2] SigmaCountyTime; // covar for county:time
    cov_matrix[2] SigmaRaceCountyTime; // covar for county:time:race



  }
  transformed parameters {
    row_vector[pDemo+6] theta[nc,2,2]; // setup a theta predictor for each observation
    for (i in 1:nc){
      theta[i,1,1,1:pDemo] = beta_RaceDemo; // black year 1
      theta[i,1,2,1:pDemo] = beta_RaceDemo; // black year 2
      theta[i,2,1,1:pDemo] = beta_RaceDemo; // white year 1
      theta[i,2,2,1:pDemo] = beta_RaceDemo; // white year 2

      theta[i,1,1,pDemo+1] = 1; // population (offset)
      theta[i,1,2,pDemo+1] = 1; // population (offset)
      theta[i,2,1,pDemo+1] = 1; // population (offset)
      theta[i,2,2,pDemo+1] = 1; // population (offset)

      theta[i,1,1,pDemo+2] = beta_Race; // race (black)
      theta[i,1,2,pDemo+2] = beta_Race; // race (black)
      theta[i,2,1,pDemo+2] = beta_Race; // race (black)
      theta[i,2,2,pDemo+2] = beta_Race; // race (black)

      theta[i,1,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,1,2,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,1,pDemo+3] = beta_Int; // intercept (global)
      theta[i,2,2,pDemo+3] = beta_Int; // intercept (global)

      theta[i,1,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,1,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta
      theta[i,2,1,pDemo+4] = beta_CountyTime[i,1]; // county:time specific beta
      theta[i,2,2,pDemo+4] = beta_CountyTime[i,2]; // county:time specific beta

      theta[i,1,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,1,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta
      theta[i,2,1,pDemo+5] = beta_RaceCountyTime[i,1]; // county:time:race specific beta
      theta[i,2,2,pDemo+5] = beta_RaceCountyTime[i,2]; // county:time:race specific beta

      theta[i,1,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,1,2,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,1,pDemo+6] = beta_Data; // fx of data source
      theta[i,2,2,pDemo+6] = beta_Data; // fx of data source
    }



  }
  model {
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          C[i,j,k] ~ poisson_log(theta[i,j,k] * x[i,j,k]);
        }
      }
    }
    beta_CountyTime ~ multi_normal(rep_row_vector(0,2),SigmaCountyTime);
    beta_RaceCountyTime ~ multi_normal(rep_row_vector(0,2),SigmaRaceCountyTime);
    }

  generated quantities {
    real RR[nc,2]; // relative risks (B/W) by year
    real<lower=0> lambda[nc,2,2] ; // lambda defines the poisson
    for (i in 1:nc){
      for (j in 1:2){
        for (k in 1:2){
          lambda[i,j,k] = exp(theta[i,j,k] * x[i,j,k]);
        }
      }
    }

    for (i in 1:nc){
      for (k in 1:2){
        RR[i,k] = lambda[i,1,k] / lambda[i,2,k];
      }
    }
  }
#+END_SRC

#+BEGIN_SRC R :session :noweb yes :results none
  stanmodel <- '
  <<stan-lme3>>
  '
#+END_SRC

*** R Code

#+BEGIN_SRC R :session
  stanprep <- unarmed.counts[complete.cases(unarmed.counts),]
  stanprep <- unarmed.counts
  stanprep$log.BW.AssaultRatio <- log(stanprep$BW.AssaultRatio)
  stanprep$log.BW.PopRatio <- log(stanprep$BW.PopRatio)

  temp <- stanprep[,c('log.BW.PopRatio','log.BW.AssaultRatio'),drop=FALSE]
  mask <- apply(temp,1,function(x){all(is.finite(x))})
  stanprep <- stanprep[mask,]


  standata <- list(
      nc = nrow(stanprep),
      Cb1 = stanprep$B.uspsd,
      Cb2 = stanprep$B.wapo,
      Cw1 = stanprep$W.uspsd,
      Cw2 = stanprep$W.wapo,
      pDemo = 2,
      xDemo = with(stanprep,cbind(log(BW.PopRatio),log(BW.AssaultRatio))),
      xOffset = cbind(stanprep$BlackPop,stanprep$WhitePop)
  )





  #fit <- stan(model_code=stanmodel, data = standata,thin = 1,iter=1000, warmup = 100, chains = 1, refresh = 10)
  fit3 <- stan(model_code=stanmodel, data = standata,thin = 1,iter=4000, warmup = 2000, chains = 1, refresh = 10)

  ex3 <- extract(fit3)


#+END_SRC
** Summarize Models
#+BEGIN_SRC R :session
  exp(mean(ex1$beta_Race))
  exp(mean(ex2$beta_Race))
  exp(mean(ex3$beta_Race))

  mean(ex1$SigmaRaceCountyTime[,1,1])
  mean(ex2$SigmaRaceCountyTime[,1,1])
  mean(ex3$SigmaRaceCountyTime[,1,1])


  mean(ex1$SigmaRaceCountyTime[,1,2])
  mean(ex2$SigmaRaceCountyTime[,1,2])
  mean(ex3$SigmaRaceCountyTime[,1,2])

#+END_SRC
